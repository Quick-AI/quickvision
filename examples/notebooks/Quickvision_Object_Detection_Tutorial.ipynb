{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Quickvision_Object_Detection_Tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a8452407d9f40149cb645cbf715e552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1ea0d354e9fd45dca0fb76debac986cb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_923651118f4b464aba524577ac7e9e70",
              "IPY_MODEL_249077eb8beb4ee081a92890adb1737f"
            ]
          }
        },
        "1ea0d354e9fd45dca0fb76debac986cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "923651118f4b464aba524577ac7e9e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_07e68125e11443058c5c235f77eb4c4a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 178728960,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 178728960,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f0c155a5f52419abeb74ba50673eb3e"
          }
        },
        "249077eb8beb4ee081a92890adb1737f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d6a7bfb4f0a548a6ba7a10780440d1c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170M/170M [00:14&lt;00:00, 12.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fd714229de9c439c9edc49c78a0e882d"
          }
        },
        "07e68125e11443058c5c235f77eb4c4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f0c155a5f52419abeb74ba50673eb3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6a7bfb4f0a548a6ba7a10780440d1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fd714229de9c439c9edc49c78a0e882d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Sd4jlGp2eLm"
      },
      "source": [
        "## Defining the Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p3vqQegzk1N"
      },
      "source": [
        "The dataset should inherit from the standard `torch.utils.data.Dataset` class, and implement `__len__` and `__getitem__`.\n",
        "\n",
        "The only specificity that we require is that the dataset `__getitem__` should return:\n",
        "\n",
        "* image: a PIL Image of size (H, W)\n",
        "* target: a dict containing the following fields\n",
        "    * `boxes` (`FloatTensor[N, 4]`): the coordinates of the `N` bounding boxes in `[x0, y0, x1, y1]` format, ranging from `0` to `W` and `0` to `H`\n",
        "    * `labels` (`Int64Tensor[N]`): the label for each bounding box\n",
        "    * `image_id` (`Int64Tensor[1]`): an image identifier. It should be unique between all the images in the dataset, and is used during evaluation\n",
        "    * `area` (`Tensor[N]`): The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.\n",
        "    * `iscrowd` (`UInt8Tensor[N]`): instances with `iscrowd=True` will be ignored during evaluation.\n",
        "\n",
        "Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a `get_height_and_width` method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via `__getitem__` , which loads the image in memory and is slower than if a custom method is provided.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX0rqK-A3Nbl"
      },
      "source": [
        "## Writing a custom dataset for Penn-Fudan\n",
        "\n",
        "Let's write a dataset for the Penn-Fudan dataset.\n",
        "\n",
        "First, let's download and extract the data, present in a zip file at https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t4TBwhHTdkd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de0366a-9c38-4251-d080-e6e25afe95a6"
      },
      "source": [
        "%%shell\n",
        "\n",
        "# download the Penn-Fudan dataset\n",
        "wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip .\n",
        "# extract it in the current folder\n",
        "unzip PennFudanPed.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-26 18:25:41--  https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\n",
            "Resolving www.cis.upenn.edu (www.cis.upenn.edu)... 158.130.69.163, 2607:f470:8:64:5ea5::d\n",
            "Connecting to www.cis.upenn.edu (www.cis.upenn.edu)|158.130.69.163|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53723336 (51M) [application/zip]\n",
            "Saving to: â€˜PennFudanPed.zip.2â€™\n",
            "\n",
            "PennFudanPed.zip.2  100%[===================>]  51.23M   216MB/s    in 0.2s    \n",
            "\n",
            "2020-11-26 18:25:42 (216 MB/s) - â€˜PennFudanPed.zip.2â€™ saved [53723336/53723336]\n",
            "\n",
            "--2020-11-26 18:25:42--  http://./\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address â€˜.â€™\n",
            "FINISHED --2020-11-26 18:25:42--\n",
            "Total wall clock time: 0.3s\n",
            "Downloaded: 1 files, 51M in 0.2s (216 MB/s)\n",
            "Archive:  PennFudanPed.zip\n",
            "replace PennFudanPed/added-object-list.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfwuU-jI3j93"
      },
      "source": [
        "Let's have a look at the dataset and how it is layed down.\n",
        "\n",
        "The data is structured as follows\n",
        "```\n",
        "PennFudanPed/\n",
        "  PedMasks/\n",
        "    FudanPed00001_mask.png\n",
        "    FudanPed00002_mask.png\n",
        "    FudanPed00003_mask.png\n",
        "    FudanPed00004_mask.png\n",
        "    ...\n",
        "  PNGImages/\n",
        "    FudanPed00001.png\n",
        "    FudanPed00002.png\n",
        "    FudanPed00003.png\n",
        "    FudanPed00004.png\n",
        "```\n",
        "\n",
        "Here is one example of an image in the dataset, with its corresponding instance segmentation mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDjuVFgexFfh"
      },
      "source": [
        "from PIL import Image\n",
        "Image.open('PennFudanPed/PNGImages/FudanPed00001.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Ee5NV54Dmj"
      },
      "source": [
        "So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let's write a `torch.utils.data.Dataset` class for this dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTgWtixZTs3X"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images ad masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        # note that we haven't converted the mask to RGB,\n",
        "        # because each color corresponds to a different instance\n",
        "        # with 0 being background\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        mask = np.array(mask)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = np.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = mask == obj_ids[:, None, None]\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        num_objs = len(obj_ids)\n",
        "        boxes = []\n",
        "        for i in range(num_objs):\n",
        "            pos = np.where(masks[i])\n",
        "            xmin = np.min(pos[1])\n",
        "            xmax = np.max(pos[1])\n",
        "            ymin = np.min(pos[0])\n",
        "            ymax = np.max(pos[0])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
        "\n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6f3ZOTJ4Km9"
      },
      "source": [
        "That's all for the dataset. Let's see how the outputs are structured for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEARO4B_ye0s"
      },
      "source": [
        "dataset = PennFudanDataset('PennFudanPed/')\n",
        "dataset[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWOhcsir9Ahx"
      },
      "source": [
        "So we can see that by default, the dataset returns a `PIL.Image` and a dictionary\n",
        "containing several fields, including `boxes`, `labels` and `masks`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l79ivkwKy357"
      },
      "source": [
        "import utils\n",
        "import transforms as T\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        # during training, randomly flip the training images\n",
        "        # and ground-truth for data augmentation\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YFJGJxk6XEs"
      },
      "source": [
        "## Putting everything together\n",
        "\n",
        "We now have the dataset class, the models and the data transforms. Let's instantiate them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdIbULSCyHK9"
      },
      "source": [
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5dGaIezze3y"
      },
      "source": [
        "# use our dataset and defined transformations\n",
        "dataset = PennFudanDataset('PennFudanPed', get_transform(train=True))\n",
        "dataset_test = PennFudanDataset('PennFudanPed', get_transform(train=False))\n",
        "\n",
        "# split the dataset in train and test set\n",
        "torch.manual_seed(1)\n",
        "indices = torch.randperm(len(dataset)).tolist()\n",
        "dataset = torch.utils.data.Subset(dataset, indices[:-50])\n",
        "dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])\n",
        "\n",
        "# define training and validation data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
        "    collate_fn=collate_fn)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "    collate_fn=collate_fn)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atAa7HsGuvFf"
      },
      "source": [
        "print(dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5yvZUprj4ZN"
      },
      "source": [
        "Now let's instantiate the model and the optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAd56lt4kDxc"
      },
      "source": [
        "And now let's train the model for 10 epochs, evaluating at the end of every epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyDuDck-ySQs"
      },
      "source": [
        "# Quickvision to Train Detection Models !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KddG4l2nyTaP"
      },
      "source": [
        "!pip install -q quickvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16Prtn800C1W"
      },
      "source": [
        "from quickvision.models.detection import faster_rcnn\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "0a8452407d9f40149cb645cbf715e552",
            "1ea0d354e9fd45dca0fb76debac986cb",
            "923651118f4b464aba524577ac7e9e70",
            "249077eb8beb4ee081a92890adb1737f",
            "07e68125e11443058c5c235f77eb4c4a",
            "0f0c155a5f52419abeb74ba50673eb3e",
            "d6a7bfb4f0a548a6ba7a10780440d1c8",
            "fd714229de9c439c9edc49c78a0e882d"
          ]
        },
        "id": "sl_20lRCmv5Y",
        "outputId": "76b5c923-eda2-4430-faf2-8af6eb7fba75"
      },
      "source": [
        "backbone = faster_rcnn.create_fastercnn_backbone(\"resnet101\", fpn=True, pretrained=\"imagenet\", trainable_backbone_layers=3)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resnet FPN Backbones works only for imagenet weights\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a8452407d9f40149cb645cbf715e552",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA_D75TYm6EL"
      },
      "source": [
        "# our dataset has two classes only - background and person\n",
        "num_classes = 2\n",
        "model = faster_rcnn.create_vision_fastercnn(num_classes=num_classes, backbone=backbone)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9zEatFBoDrL"
      },
      "source": [
        "- Quickvision supports Mixed Precision training as well !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT1wE4s5oBXO"
      },
      "source": [
        "# Let's use Mixed Precision training\n",
        "from torch.cuda import amp"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNsnwJc3nLRJ"
      },
      "source": [
        "scaler = amp.GradScaler()\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# and a learning rate scheduler which decreases the learning rate by 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9nxh28en0fP"
      },
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    train_metrics = faster_rcnn.train_step(model, train_loader, device=\"cuda\", optimizer=optimizer, scheduler=lr_scheduler, log_interval=100, scaler=scaler)\n",
        "    val_metrics = faster_rcnn.val_step(model, test_loader, device=\"cuda\")\n",
        "    print(\"Training Metrics: \")\n",
        "    pprint(train_metrics)\n",
        "    print(\"Validation metrics\")\n",
        "    pprint(val_metrics)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86hrzxTfyNZL"
      },
      "source": [
        ""
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}