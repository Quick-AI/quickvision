{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNs_with_Quickvision.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5550f3d1dfde4167b626b131301c68ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e500c6fca9294128aaf3f6aeaa9e7761",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_19837298bb224b1a9801598ecf9c0119",
              "IPY_MODEL_13ec583e680d423486bcb1d4b05b856f"
            ]
          }
        },
        "e500c6fca9294128aaf3f6aeaa9e7761": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19837298bb224b1a9801598ecf9c0119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2961e945a2af4360a5333d17d4956928",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f7d76c8d02d1493eb94d3fd722cfeb85"
          }
        },
        "13ec583e680d423486bcb1d4b05b856f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_72922b6b05d742788aa46f27badb54f1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:19&lt;00:00, 33514292.56it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69c46b1ed258418ab50ee49a268b90e6"
          }
        },
        "2961e945a2af4360a5333d17d4956928": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f7d76c8d02d1493eb94d3fd722cfeb85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "72922b6b05d742788aa46f27badb54f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69c46b1ed258418ab50ee49a268b90e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyxgU7hwBN4K"
      },
      "source": [
        "## What is Quickvision ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GKAHq41DO4p"
      },
      "source": [
        "- Quickvision is a Computer Vision Library built on Top of Torchvision, PyTorch and Lightning\n",
        "\n",
        "A brief description on how it works.\n",
        "\n",
        "- It provides Easy to use torch native API, for fit(), train_step(), val_step() of models.\n",
        "\n",
        "- It is Easily customizable and configurable models with various backbones.\n",
        "\n",
        "- A complete torch native interface. All models are nn.Module all the training APIs are optional and not binded to models.\n",
        "\n",
        "- Tensor First library. No abstraction and classes over it ! \n",
        "\n",
        "- A lightning API which helps to accelerate training over multiple GPUs, TPUs.\n",
        "\n",
        "- A datasets API to common data format very easily and quickly to torch formats.\n",
        "\n",
        "- A minimal package, with very low dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wawtpa3CDq4-"
      },
      "source": [
        "### Let's explore these with a simple Image Classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BHtveDiD2b9"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "memMtSeqD3d9"
      },
      "source": [
        "Install directly from GitHub. Very soon it will be available over PyPi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Kb2KnJiBBQF",
        "outputId": "49b1ce80-747c-47c2-8df8-f80466485e01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install -q git+https://github.com/Quick-AI/quickvision.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for quickvision (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8PxnXi0E7tF"
      },
      "source": [
        "## Some imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKK7lmkOE80m"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "from tqdm import tqdm\n",
        "import pytorch_lightning as pl\n",
        "# And finally our Hero !\n",
        "import quickvision"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2X9FYlBEodG"
      },
      "source": [
        "## Create a dataset and dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeLHW85wEr2l"
      },
      "source": [
        "- This process is completely as in PyTorch.\n",
        "\n",
        "- For example let's take the CIFAR10 dataset available in torchvision.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnhsjy-wFGfH"
      },
      "source": [
        "### Training and testing transfomrs\n",
        "\n",
        "Quickivision is not binded to any library for transforms.\n",
        "\n",
        "Let's keep it simple and use torchvision transforms for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1q51DuPFFpF"
      },
      "source": [
        "# Train and validation Transforms which you would like\n",
        "train_transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])\n",
        "valid_transforms = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBr0O1r7ES8W",
        "outputId": "03a64b07-5264-4b3e-b7f1-2605610f40a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "5550f3d1dfde4167b626b131301c68ef",
            "e500c6fca9294128aaf3f6aeaa9e7761",
            "19837298bb224b1a9801598ecf9c0119",
            "13ec583e680d423486bcb1d4b05b856f",
            "2961e945a2af4360a5333d17d4956928",
            "f7d76c8d02d1493eb94d3fd722cfeb85",
            "72922b6b05d742788aa46f27badb54f1",
            "69c46b1ed258418ab50ee49a268b90e6"
          ]
        }
      },
      "source": [
        "train_set = torchvision.datasets.CIFAR10(\"./data\", download=True, train=True, transform=train_transforms)\n",
        "valid_set = torchvision.datasets.CIFAR10(\"./data\", download=True, train=False, transform=valid_transforms)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5550f3d1dfde4167b626b131301c68ef",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKNDI7c2FYU3"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=32, shuffle=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4pnUs0KFqU6"
      },
      "source": [
        "## Creating model !!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0mdPX8VFzB4"
      },
      "source": [
        "- Quickvision Provides simple functions to create models with pretrained weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_ljlgF2FyE4"
      },
      "source": [
        "from quickvision.models.classification import cnn"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OURry2JrGAJl"
      },
      "source": [
        "# To create model with imagenet pretrained weights \n",
        "model = cnn.create_vision_cnn(\"resnet50\", num_classes=10, pretrained=\"imagenet\")\n",
        "\n",
        "# Alternatively if you don't need pretrained weights\n",
        "model_bare = cnn.create_vision_cnn(\"resnet50\", num_classes=10, pretrained=None)\n",
        "\n",
        "# It also supports other weights, do check a list which are supported !\n",
        "\n",
        "model_ssl = cnn.create_vision_cnn(\"resnet50\", num_classes=10, pretrained=\"ssl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8RZJ_4SG2_o"
      },
      "source": [
        "### Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5HX4EtBG4uO"
      },
      "source": [
        "- Again this is just like in torch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwVXfhsjG20Q"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJa724TuH2a-"
      },
      "source": [
        "## Training "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq6xJJgWGoDu"
      },
      "source": [
        "### The usual boring PyTorch procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE3RAwLaLC-j"
      },
      "source": [
        "- This too works with Quickvision, we just have models as `nn.Module` .\n",
        "\n",
        "It is same way as you would train a CNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsrahKZrFmoz"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFTyhOYIF-J4",
        "outputId": "99bbd549-370e-4bba-d93a-b0fa33907b87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = model.to(device)\n",
        "for epoch in range(2):\n",
        "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        target = target.to(device)\n",
        "        out = model(inputs)\n",
        "        loss = criterion(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "print(\"Done !!\")\n",
        "# And the boring boiler plate continues\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done !!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNsunzCzHupY"
      },
      "source": [
        "### The quickvision way of doing it !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O5XVd9tH5Dh"
      },
      "source": [
        "- We have already implemented these boring procedures for you to speed up training !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp8gRByvHmLh"
      },
      "source": [
        "model = cnn.create_vision_cnn(\"resnet50\", num_classes=10, pretrained=\"imagenet\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwoHZL9MIpwH"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLdfd3YuILaJ"
      },
      "source": [
        "history = cnn.fit(model=model, epochs=2, train_loader=train_loader,\n",
        "        val_loader=valid_loader, criterion=criterion, device=device, optimizer=optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDxBb1--I2QE"
      },
      "source": [
        "- If you need a granular control over the training.\n",
        "\n",
        "You can use `train_step` and `val_step` methods.\n",
        "\n",
        "We calculate commonly used metrics such as `accuracy` here for you !\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKpYJvzZItS0"
      },
      "source": [
        "for epoch in tqdm(range(2)):\n",
        "    print()\n",
        "    print(f\"Training Epoch = {epoch}\")\n",
        "    train_metrics = cnn.train_step(model, train_loader, criterion, device, optimizer)\n",
        "    print()\n",
        "\n",
        "    print(f\"Validating Epoch = {epoch}\")\n",
        "    valid_metrics = cnn.val_step(model, valid_loader, criterion, device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jz9neAdKMoG"
      },
      "source": [
        "### Train with Lightning !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca6on8sMKQKv"
      },
      "source": [
        "- We have the same logics implemented for PyTorch Lightning as well.\n",
        "- This directly allows you to use all Lighning features such as Multi-GPU training, TPU Training, logging etc.\n",
        "\n",
        "Quickly Prototype with Torch, transfer it to Lightning !\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPLTOD4rKO69"
      },
      "source": [
        "model_imagenet = cnn.lit_cnn(\"resnet18\", num_classes=10, pretrained=\"imagenet\")\n",
        "\n",
        "gpus = 1 if torch.cuda.is_available() else 0\n",
        "\n",
        "# Again use all possible Trainer Params from Lightning here !!\n",
        "trainer = pl.Trainer(gpus=gpus, max_epochs=2)\n",
        "trainer.fit(model_imagenet, train_loader, valid_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRAsNz4TLxUY"
      },
      "source": [
        "## Some more Features of Quickvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mv36qVgNL0XZ"
      },
      "source": [
        "- Support for mixed precision training in PyTorch API over GPUs.\n",
        "\n",
        "- Supports Detection models such as `Detr`, `FRCNN`, `RetinaNet`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Riy8NgHqJiAY"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Cr6NrqJmTN"
      },
      "source": [
        "1. Quickvision allows you to bring your own `Dataset`, `Model` or `Code` Recipe\n",
        "\n",
        "2. You may use models, training functions or Dataset loading utilites from quickvision.\n",
        "\n",
        "3. Seamless API to connect with Lightning as well.\n",
        "\n",
        "4. Faster Experimentation with same control with PyTorch or Lightning.\n",
        "\n",
        "Visit us [here](https://github.com/Quick-AI/quickvision) over GitHub !\n",
        "\n",
        "We are happy for new contributions / improvements to our package.\n",
        "\n",
        "Quickivison is a library built for faster but no compromise PyTorch Training !\n",
        "\n"
      ]
    }
  ]
}